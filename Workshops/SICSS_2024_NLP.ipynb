{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PQFfQtDKjH5QUvkNHzO9bQl5a0Z1uxuj","timestamp":1718564460895}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SICSS-AMU/Law 2024\n","## June 17, 2024 | Adam Mickiewicz University\n","![](https://drive.google.com/uc?export=view&id=1-XuMzYGaD6X4dfFxA_fqaJAWzub8S7ky)\n","#### prepared by Mateusz Mroziński\n","[Github](https://github.com/gepetto2)"],"metadata":{"id":"IUnNxxzKaPPj"}},{"cell_type":"markdown","source":["# Abstract\n","\n"],"metadata":{"id":"wi7nEze1kS9d"}},{"cell_type":"markdown","source":["**Natural Language Processing** is a field in computer science (frequently considered also a branch of Artificial Intelligence) that focus on manipulating and extrating information from text written in human languages.\n","\n","As humans, we have no problem understanding, for example, this text. Unfortunately, computers cannot do the same - they operate on numbers, values and equations. Despite that, with our help and a bit of configuration, we can try to use computers to extract some statistical informations, manipulate text and - with help of new and advanced language models - even summarize a text portion and many more!\n","\n","Colab supports two programming languages - R and Python. We will use the latter, which is perfect for Natural Language Processing, thanks to a large number of existing libraries - packages of code covering a lot of NLP tasks. This will make our task a lot easier, as we don't have to build the code from the ground."],"metadata":{"id":"XVPBoxmvzVws"}},{"cell_type":"markdown","source":["#Natural Language Processing"],"metadata":{"id":"bREOXfzqb9Xg"}},{"cell_type":"markdown","source":["**Before we start...**\n","\n","Please start by copying this notebook to your own Drive. It will allow you to edit the code and save your progress. To do this, click the Copy to Drive button.\n","\n","Next, click the Connect button to connect to external machine. Google Colabs are very convenient - they work in the cloud, so you don't need to install all the requirements to your own computer. If you see a green checkmark ✔️, you're ready to start working!\n","\n","Code in this notebook is divided into many fragments, called cells. You can run every code cell by clicking Run button ▷ on the left side. The output of the code (if there's any) will appear below the code cell.\n","\n","Notice, the code contains a lot of green lines preceded by # sign. These are called comments - they do not impact the output, they are only there for you to understand the code better.\n","\n","You can try the one below!\n"],"metadata":{"id":"e5-RBtdiNQMV"}},{"cell_type":"code","source":["# Let's display some simple text\n","print(\"Hello world!\")"],"metadata":{"id":"CvaUGSHNhVdz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ❗ Please run this code ❗ It's important for better text formatting\n","# ----------------------------------------------------------------------------------------\n","# You can ignore this code, it's only needed so the displayed text looks better\n","from IPython.display import HTML, display\n","\n","deepl_key = '0a3beb35-1d5a-4dae-b20f-a31fb8396ed8:fx'\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"9DSdwX3ZE58Q","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Obtaining legal documents and pre-processing"],"metadata":{"id":"0N60ukReHo5u"}},{"cell_type":"markdown","source":["To present capabilities of Natural Language Processing, we need a text source to process. [EUR-Lex ](https://eur-lex.europa.eu/homepage.html) portal is perfect for that. It's an online database of European Union's legal documents. It's available in different languages and is updated regularly. If you want to know more, [here's a short video](https://www.youtube.com/watch?v=1k7FXMO-W6U) about it.\n","\n","Now we need to obtain actual text content. We'll use **web scraping** - it's a process of obtaining information from websites. There are a lot of ways to do that. We will use external libraries: `requests` to access the document and `BeautifulSoup` to prepare it for NLP.\n","\n","Usually, before importing libraries you need to **install** them first, but fortunately Colab notebooks come with a package of popular libraries, so these ones are already installed.\n","\n","We need to import them first - prepare them for later use. Let's do it now:"],"metadata":{"id":"AEk8vAVxnGfi"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup"],"metadata":{"id":"sPMh2jX5_RaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We haven't decided yet what exactly we want to work with! Let's analyse one of documents in the Artificial Intelligence Act. If you want, you can check it out [here](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206)."],"metadata":{"id":"01zmsiiDTs4o"}},{"cell_type":"markdown","source":["Before we start actual coding, let's briefly explain some fundamentals:\n","\n","**Variables** are pieces of whatever information we need to use - numbers, text, lists and many other objects. You can think of them as boxes, to which you can put something and computer will remember that and let you use it later.\n","\n","![](https://drive.google.com/uc?export=view&id=182quhb1eS_-S5g5uzXfN2R7HktWz6cmC)\n","\n","A **function** is a block of code that performs a specific task; it usually takes some arguments, performs some transformations or calculations using them and returns a result. You can define your own functions, but in our case we will use existing functions from external libraries, so you only need to know how to **call** existing functions.\n","\n","![](https://drive.google.com/uc?export=view&id=1P03cF13KGcOTuU6LL6B63U2L4FwWJpA6)"],"metadata":{"id":"RQjEVbZsiaR2"}},{"cell_type":"code","source":["# We specify URL address we want to\n","\n","URL_address = \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206\"\n","page_source = requests.get(URL_address)"],"metadata":{"id":"l0ygFxv3_KV0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we already did the first step, why not try to see our result!"],"metadata":{"id":"0PDDN6v0zZSn"}},{"cell_type":"code","source":["# Let's display the first 10000 characters of our text\n","print(page_source.text[:10000])"],"metadata":{"id":"7y73MEsRzYqD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, it's a lot of mess. That's because we copied not only raw text, but whole source of a page with a lot of complicated elements we don't need. So now we need to clean it up - that's where `BeautifulSoup` will be helpful."],"metadata":{"id":"_-BDxso6zq1L"}},{"cell_type":"code","source":["# Let's convert our page to BeautifulSoup object\n","\n","soup = BeautifulSoup(page_source.content, \"html.parser\")\n","\n","# It already should look better, but let's do one more thing - remove footnote references\n","\n","for element in soup.find_all('span', {'class':'FootnoteReference'}):\n","    element.decompose()"],"metadata":{"id":"mQaSyZuEwN2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try to display text now:"],"metadata":{"id":"781aXijw7m9z"}},{"cell_type":"code","source":["# Let's display the first 10000 characters of our text again\n","print(soup.get_text(strip=True)[:10000])"],"metadata":{"id":"T50MYTCl5OSP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks a lot better! Maybe for human it's a little hard to read, as it's not divided into paragraphs, but for a computer it's good enough.\n","\n","As the document is very long, in some of the excercises we will focus only on the first chapter. We will create two **variables** - `full_text` and `short_text`."],"metadata":{"id":"Q-JL3q35xAp1"}},{"cell_type":"code","source":["full_text = soup.get_text(strip=True)\n","\n","# Here's code to extract only the first chapter - don't worry if it looks complicated\n","\n","short_text = \"\"\n","\n","for element in soup.find_all(\"p\", {'class':['Normal', 'li ListBullet1']})[0:24]:\n","    short_text = short_text + element.get_text(strip=True) + '\\n'"],"metadata":{"id":"BMNJtPji5LjT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Searching and filtering using Regular Expressions"],"metadata":{"id":"sHpVTpAjk8Zb"}},{"cell_type":"markdown","source":["**Regex**, short for **Regular Expression** is basically a pattern built with a sequence of characters. It is used widely in many different tasks, for example:\n","*   Filtering and searching\n","*   Validating user input (for example in web forms)\n","*   Replacing specific sequences with other ones\n","\n","Regex is composed of tokens - you could call them placeholders for things (characters, words etc.) we will be searching for. For example `\\d` means any digit. They can be combined into groups using square brackets. There are also quantifiers, which are added after a token in curly brackets.\n","\n","Here's a little cheat sheet explaining basics of Regex. At any point you can come back here for help\n","\n","![](https://drive.google.com/uc?export=view&id=1FCmJKAgRYpA0B3azUBbtlxfoJh9t4ram)\n","\n","There's a lot more to it - Regex is a complicated tool and even advanced programmers can struggle with remembering all the rules - don't worry if you don't fully understand it at first. [Here's a great resource](https://regexr.com/) for learning and experimenting - you can check it out!\n","\n"],"metadata":{"id":"eI4GoVL6VT82"}},{"cell_type":"markdown","source":["Let's start by importing `re` library"],"metadata":{"id":"iJxagwThLFzT"}},{"cell_type":"code","source":["import re"],"metadata":{"id":"Eo_jMtnN_7Mn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try to find every money amount mentioned in the document using Regex. Assuming the format is `<amount> EUR`, for example `1000 EUR`, our Regular Expression could look like this:\n","\n","`[\\d ]*\\d EUR`\n","\n"],"metadata":{"id":"Ea6qNJmDK2kz"}},{"cell_type":"markdown","source":["Let's break it down into parts:\n","\n","\n","* `[\\d ]*` - any combination of digits and spaces\n","* `\\d` - one digit\n","* ` ` - one space\n","* `EUR` - literally letters \"EUR\"\n","\n"],"metadata":{"id":"SfycivgThsWE"}},{"cell_type":"code","source":["matches_list =  re.findall(r'[\\d ]*\\d EUR', full_text)\n","\n","for single_match in matches_list:\n","  print(single_match)"],"metadata":{"id":"WfPjB4hsAHVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's working - we found some matches, but it would be nice to have some context. First, let's specify how much (in terms of character count) context we want to see:"],"metadata":{"id":"DZ4oXj798Xld"}},{"cell_type":"code","source":["left_context = 60\n","right_context = 60"],"metadata":{"id":"Ca89oQB2kyrW","executionInfo":{"status":"ok","timestamp":1718638049350,"user_tz":-120,"elapsed":8,"user":{"displayName":"AMU Law SICSS","userId":"08370092216492357083"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Now, we'll iterate over matches list and print them, this time including some context"],"metadata":{"id":"MTF7-tQ0k2B9"}},{"cell_type":"code","source":["for single_match in matches_list:\n","    # Find the starting position of the match in the full_text\n","    start_position = full_text.find(single_match)\n","\n","    # Calculate the start and end positions of the context\n","    context_start = start_position - left_context\n","    context_end = start_position + len(single_match) + right_context\n","\n","    # Extract the context from the full_text\n","    context = full_text[context_start : context_end]\n","\n","    print(context)"],"metadata":{"id":"Mzq75ltX8ER0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Most frequent words and wordcloud"],"metadata":{"id":"H_NjA8MBxIf_"}},{"cell_type":"markdown","source":["Common way of analyzing text is compiling most frequent words. We'll use `Counter` from `collections` library to count how many each word appears in the text, then `pandas` and `matplotlib` to convert it into graphic chart."],"metadata":{"id":"Bb9BXSTwxOiO"}},{"cell_type":"code","source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Tokenize the text into words\n","words = full_text.split()\n","\n","# Create a counter of word frequencies in the text, converting all words to lowercase\n","word_counts = Counter(word.lower() for word in words)\n","\n","# Create a DataFrame with the 15 most common words and their counts\n","word_freq = pd.DataFrame(word_counts.most_common(15), columns=['words', 'count'])\n","\n","# Plot the word frequencies as a horizontal bar chart\n","word_freq.sort_values(by='count').plot.barh(x='words', y='count', figsize=(12, 8))"],"metadata":{"id":"WeHdNImbZckU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, there's one problem with our plot - it consists mostly of very common English words, like \"the\", \"and\" etc. These words are called **stopwords** and they don't really give us any useful information about the text. We want to eliminate them.\n","\n","We will repeat the previous code, but this time during counting we'll check if given word is a **stopword**."],"metadata":{"id":"da4ytXlK4GRy"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","\n","# Download stopwords\n","\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","# Tokenize the text into words\n","words = full_text.split()\n","\n","# Count word frequencies excluding stopwords\n","word_counts = Counter(\n","    word.lower() for word in words if word.lower() not in stop_words\n",")\n","\n","# Create a DataFrame with the 15 most common words and their counts\n","word_freq = pd.DataFrame(word_counts.most_common(15), columns=['words', 'count'])\n","\n","# Plot the word frequencies as a horizontal bar chart\n","word_freq.sort_values(by='count').plot.barh(x='words', y='count', figsize=(12, 8))"],"metadata":{"id":"0MUn5ygwezjW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can often see most frequent words visualized as a \"word-cloud\" - a graphical collage of words, where the most frequent and important are the biggest and vice-versa. We'll use `wordcloud` library. Let's install it first."],"metadata":{"id":"IxuHrErCAbLl"}},{"cell_type":"code","source":["!pip install wordcloud"],"metadata":{"id":"5tjMF0OmGsum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The use of this library is pretty straightforward:"],"metadata":{"id":"9YPxy2Qdbss9"}},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","cloud = WordCloud(\n","    background_color='white',\n","    width=1600,\n","    height=1000,\n","    collocations=False\n",").generate(full_text)\n","\n","plt.figure(figsize=(20,10))\n","plt.axis(\"off\")\n","plt.imshow(cloud)"],"metadata":{"id":"MKC5jT1wG0Gc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Machine translation"],"metadata":{"id":"LWHwLFE76yJA"}},{"cell_type":"markdown","source":["One of the most common text transformations is translating it to a different language. Of course best result is achieved by manual translation by a human, but sometimes, when it's necessary, machine translation is a good alternative. We will use `deep-translator` library. Let's install it."],"metadata":{"id":"6yTTyxlgU188"}},{"cell_type":"code","source":["!pip install -U deep-translator"],"metadata":{"id":"qUaT8b81J-2s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Library `deep-translator` offers services of top translators, such as Google Translate, DeepL, Microsoft Translator, ChatGPT and many many others. To access some of them, you'll need an API key, which isn't always free to obtain. For example - DeepL translator offers a free API key up to 500 000 translated characters every month."],"metadata":{"id":"PRqjEGLxMl4p"}},{"cell_type":"markdown","source":["Let's try translating text using DeeplTranslator"],"metadata":{"id":"DFORvzMYSB3u"}},{"cell_type":"code","source":["from deep_translator import DeeplTranslator\n","\n","translated1 = DeeplTranslator(api_key=deepl_key, source=\"english\", target=\"polish\", use_free_api=True).translate(short_text[:4359])\n","\n","print(translated1)"],"metadata":{"id":"vMAFAA0bKSaS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's try a different one, GoogleTranslator"],"metadata":{"id":"KaazPek6Xk28"}},{"cell_type":"code","source":["from deep_translator import GoogleTranslator\n","\n","translated2 = GoogleTranslator(source='english', target='polish').translate(short_text[:4359])\n","\n","print(translated2)"],"metadata":{"id":"bRcnOwHK1f0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Document comparison"],"metadata":{"id":"piB5PYLUnfW9"}},{"cell_type":"markdown","source":["In above example, it would be nice to compare both texts. We wil try to do it using `difflib` library"],"metadata":{"id":"Zfv-lo6UXstT"}},{"cell_type":"code","source":["from difflib import Differ\n","from IPython.display import HTML, display\n","\n","# Compare the two translated texts\n","diff = Differ().compare(translated1.split(), translated2.split())"],"metadata":{"id":"8Us7pD6g39vw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`difflib` library adds different prefix to each word, depending if said word was removed, added or nothing was changed compared to first text. Let.s create a short HTML text to display changes between texts in a nice way."],"metadata":{"id":"sC4z5jkzUQRo"}},{"cell_type":"code","source":["html_text = ''\n","\n","for element in diff:\n","    prefix = element[0]\n","    word = element[2:]\n","    if prefix == ' ':  # Unchanged\n","      html_text += word+' '\n","    elif prefix == '-':  # Removed\n","      html_text += f'<span style=\"color: red; text-decoration: line-through;\">{word} </span>'\n","    elif prefix == '+':  # Added\n","      html_text += f'<span style=\"color: lime;\">{word} </span>'\n","    elif prefix == '?':  # Separator\n","        continue  # Skip separators\n","\n","# Display the formatted HTML text\n","display(HTML(html_text))"],"metadata":{"id":"vE4JunQnUPnw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Named Entity Recognition"],"metadata":{"id":"mEVdiuhcwyjr"}},{"cell_type":"markdown","source":["A Named Entity Recognition (NER) extractor finds entities, which can be people, companies, or locations and exist within text data."],"metadata":{"id":"1uLyUHXsw4ow"}},{"cell_type":"markdown","source":["We'll use `spacy`. Let's import it."],"metadata":{"id":"RyaGPupF2vAv"}},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"ZoHPVUid23TB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Spacy has a built-in model `en_core_web_sm` but it's not very effective in our case. We'll use different one - `en_core_web_trf`, which is slower, but generates better results.\n","\n","Let's download the model, and reload session."],"metadata":{"id":"NGjUYigM3CiI"}},{"cell_type":"markdown","source":["❗ This code can take a while to finish, so be patient please ❗"],"metadata":{"id":"BTgqylrvCz3E"}},{"cell_type":"code","source":["# Download a better model for NER\n","!python -m spacy download en_core_web_trf"],"metadata":{"id":"EhY4pHhA3A2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After downloading model we need to restart the session. Please run the following cell. Don't worry if you'll see \"Your session crashed for an unknown reason\""],"metadata":{"id":"WLlrn-v5cm4y"}},{"cell_type":"code","source":["# @title ❗ Run this code cell to restart session ❗ Please ignore any error\n","import pickle\n","with open('data.pkl', 'wb') as f:\n","  pickle.dump(short_text, f)\n","  pickle.dump(full_text, f)\n","\n","import os\n","os.kill(os.getpid(), 9)"],"metadata":{"id":"OAnyIShXE1Sp","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we restarted the session, we lost our variables. Let's recover them. Just run the code cell below."],"metadata":{"id":"ALl4KoIYeBlu"}},{"cell_type":"code","source":["import pickle\n","with open('data.pkl', 'rb') as f:\n","    short_text = pickle.load(f)\n","    full_text = pickle.load(f)"],"metadata":{"id":"opKIfI-sPUIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We start by loading the model"],"metadata":{"id":"xinDRIOt-9bC"}},{"cell_type":"markdown","source":["❗ This code can take a while to finish, so be patient please ❗"],"metadata":{"id":"FCFxZM_SHWJG"}},{"cell_type":"code","source":["# Let's load the model we just downloaded\n","import spacy\n","\n","nlp = spacy.load('en_core_web_trf')\n","\n","# We will use our short text variable\n","\n","doc = nlp(short_text)\n","\n","# Spacy offers a nice way to display results - displacy.render()\n","\n","spacy.displacy.render(doc, style=\"ent\")"],"metadata":{"id":"g-HBwDCYqC9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are situations when we want just a list of entities and their types, without whole text. Let's print such list."],"metadata":{"id":"Kn2hK2-5jvlx"}},{"cell_type":"code","source":["import pandas as pd\n","from IPython.display import display\n","\n","# Extract entities and their labels\n","entities = [(ent.text, ent.label_) for ent in doc.ents]\n","\n","# Create a DataFrame from the entities\n","df = pd.DataFrame(entities, columns=['text', 'type'])\n","\n","# Display the DataFrame\n","pd.set_option('display.max_rows', None)\n","display(df)"],"metadata":{"id":"0t3lAVpOiepx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excercises"],"metadata":{"id":"gou38f2RTucn"}},{"cell_type":"markdown","source":["As for the excercises, please do similar tasks as above, with some changes (you can also experiment with a different document, if you feel like!):"],"metadata":{"id":"KA5H6UheEHYA"}},{"cell_type":"markdown","source":["**Exercise 1.**\n","\n","Regex - try to create expression, which will help you find all dates in the document in the format `DD.MM.RRRR`, with 50 characters of context."],"metadata":{"id":"Daf44siKo05G"}},{"cell_type":"code","source":["# Place your code here!"],"metadata":{"id":"6EIgzV5V9SRO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Excercise 2.**\n","\n","Perform a Named Entity Recognition, this time using the default model `en_core_web_sm`. Compare results and decide which is better."],"metadata":{"id":"DfpOqzaKo1c2"}},{"cell_type":"code","source":["# Place your code here!"],"metadata":{"id":"ch-2n9-4FGF6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Excercise 3.**\n","\n","Create Wordcloud, but exclude the named argument `collocations=False`. Plot the Wordcloud, and explain the difference - is the result better or worse?"],"metadata":{"id":"AHnqvCA8w9B6"}},{"cell_type":"code","source":["# Place your code here!\n","#HINT: Look for differences in two-letter words"],"metadata":{"id":"6rdBiHr27n9N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Excercise 4.**\n","\n","Most common words - create a bar chart of most common words in document, but change bar color to red and exclude extra words: \"european\", \"union\", \"ai\""],"metadata":{"id":"BSKxv_Pio2FJ"}},{"cell_type":"code","source":["# Place your code here!"],"metadata":{"id":"4UX8MTLpP8RV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Excercise 6.**\n","\n","Let's change the look of most frequent words chart. Change the type of chart to vertical bars, change color to red and set title to \"Most frequent words\". You can find useful information [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)"],"metadata":{"id":"YXrtqK5wRnhr"}},{"cell_type":"code","source":["# Place your code here!\n","\n","#HINT: To change type of chart from horizontal bars to vertical bars, just change function from barh to bar. Other parameters can be changed by adding arguments next to x, y and figsize."],"metadata":{"id":"Bmx3_7VmSI1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Excercise 5.**\n","\n","In the Named Entity Recognition, count how many of each entity types are in the text."],"metadata":{"id":"HMosmE6bQ9MC"}},{"cell_type":"code","source":["# Place your code here!\n","#HINT: You can use Counter, to count how many there are entities of each type in df['types']. Then, you can use display() function to show results."],"metadata":{"id":"g9so-NchRJdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 7.**\n","\n","When we analyzed most frequent words in text, we were breaking it (or tokenizing) into single words. In NLP they are also called one-grams. Similarly, sequence of 2 words is called a bigram, sequence of 3 words is called a trigram etc. This concept is common in NLP and is called **N-gram**.\n","\n","Now, as an excercise, try to create a graph of most common bigrams in the text."],"metadata":{"id":"YpfwNqIGC_31"}},{"cell_type":"code","source":["# Place your code here!\n","\n","#HINT: when having word list in the variable words, you can get list of word pairs like that:\n","# word_pairs = [(words[i], words[i+1]) for i in range(len(words) - 1)]"],"metadata":{"id":"AmJxsEO-o1tZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 8.**\n","\n","Go to [www.deepl.com/pro-api](https://www.deepl.com/pro-api). Create a free account and generate your own free API key. Use the key and `deep_translator` library to translate this instruction to a language of your choice."],"metadata":{"id":"7RoZ6Koj7p6R"}},{"cell_type":"code","source":["# Place your code here!"],"metadata":{"id":"PgQjwpOBE_qN"},"execution_count":null,"outputs":[]}]}